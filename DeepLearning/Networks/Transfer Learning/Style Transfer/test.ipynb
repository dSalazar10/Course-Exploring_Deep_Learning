{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from torchvision import transforms, models\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, content_path, style_path, target_path, epoch=2000, alpha=1, beta=1e6):\n",
    "    super(Network, self).__init__()\n",
    "    # path where the output file will be saved\n",
    "    self.target_path = target_path\n",
    "    # number of updates to image\n",
    "    self.epoch = epoch\n",
    "    # It's recommended that you leave the content_weight = 1\n",
    "    self.content_weight = alpha\n",
    "    # set the style_weight to achieve the ratio you want\n",
    "    self.style_weight = beta\n",
    "    # get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "    self.model = models.vgg19(pretrained=True).features\n",
    "    # freeze all VGG parameters since we're only optimizing the target image\n",
    "    for param in self.model.parameters():\n",
    "      param.requires_grad_(False)\n",
    "    # check if CUDA is available\n",
    "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # set model to cuda or cpu\n",
    "    self.model.to(self.device)\n",
    "    # load content image and set to cuda or cpu\n",
    "    self.content = self.load_image(content_path).to(self.device)\n",
    "    # load stylee image and set to cuda or cpu\n",
    "    self.style = self.load_image(style_path, shape=self.content.shape[-2:]).to(self.device)\n",
    "\n",
    "  def save_image(self, image):\n",
    "    \"\"\"\n",
    "    This saves the output image to disk\n",
    "    Input:\n",
    "    * image: the image data to be saved\n",
    "    \"\"\"\n",
    "    # creating a image object\n",
    "    img = Image.open(self.target_path)\n",
    "    # save a image using extension \n",
    "    return img.save('out.jpg')\n",
    "\n",
    "  def load_image(self, img_path, max_size=400, shape=None):\n",
    "    \"\"\"\n",
    "    Load in and transform an image, making sure the image\n",
    "    is <= 400 pixels in the x-y dims.\n",
    "    Input:\n",
    "    * img_path: the path to the input image\n",
    "    * max_size: (Optional) maximum image size\n",
    "    * shape: shape of image; used to make style image same shape as content image\n",
    "    Output:\n",
    "    * returns a trasnformed PIL image\n",
    "    \"\"\"\n",
    "    # if path is a url\n",
    "    if \"http\" in img_path:\n",
    "      response = requests.get(img_path)\n",
    "      image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else: # path is local\n",
    "      image = Image.open(img_path).convert('RGB')\n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "      size = max_size\n",
    "    else:\n",
    "      size = max(image.size)\n",
    "    if shape is not None:\n",
    "      size = shape\n",
    "    # resize the image, convert to a tensor, and normalize\n",
    "    in_transform = transforms.Compose([transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "  \n",
    "  def im_convert(self, tensor):\n",
    "    \"\"\"\n",
    "    Helper function for un-normalizing an image and converting\n",
    "    it from a Tensor image to a NumPy image for display\n",
    "    Input:\n",
    "    * tensor: the image tensor (content, style, target)\n",
    "    Output:\n",
    "    * returns a converted image\n",
    "    \"\"\"\n",
    "    # convert to cpu, clone the image, and detach\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    # convert to numpy array and remove single-dimensional \n",
    "    # entries from the shape of the array\n",
    "    image = image.numpy().squeeze()\n",
    "    # get a new view of the image\n",
    "    image = image.transpose(1,2,0)\n",
    "    # scale the image\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    # limit the values to a min of 0 and max of 1\n",
    "    image = image.clip(0, 1)\n",
    "    return image\n",
    "\n",
    "  def get_features(self, image, layers=None):\n",
    "    \"\"\" \n",
    "    Run an image forward through a model and get the features for \n",
    "    a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
    "    Input:\n",
    "    * image: PIL image\n",
    "    * layers: (Optional) specify a layer\n",
    "    Output:\n",
    "    * returns the features of the input image\n",
    "    \"\"\"\n",
    "    ## Mapping layer names of PyTorch's VGGNet to names from the paper\n",
    "    ## Need the layers for the content and style representations of an image\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1', \n",
    "                  '10': 'conv3_1', \n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',  ## content representation\n",
    "                  '28': 'conv5_1'}\n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in self.model._modules.items():\n",
    "        # forward pass\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n",
    "\n",
    "  def gram_matrix(self, tensor):\n",
    "    \"\"\" \n",
    "    Calculate the Gram Matrix of a given tensor \n",
    "    Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    Input:\n",
    "    * tensor: feature map matrix (style, target)\n",
    "    Output:\n",
    "    * returns the Gram Matrix of the feature map\n",
    "    \"\"\"\n",
    "    # get the batch_size, depth, height, and width of the Tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    # reshape so we're multiplying the features for each channel\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    # calculate the gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram \n",
    "\n",
    "  def transfer(self, vis=False):\n",
    "    \"\"\"\n",
    "    Get features of content and style images, calculates the gram matricies, creates\n",
    "    a target image based off of the content image, calculates to Content Loss, Calculates\n",
    "    the Style loss, calculates the Total Loss, and updates the image based on the results.\n",
    "    This will optionally display the image every 400 iterations\n",
    "    This will save the file to disk\n",
    "    Input:\n",
    "    * vis: (Optional) this controls the diplay of images\n",
    "    Output:\n",
    "    * None\n",
    "    \"\"\"\n",
    "    # get content and style features only once before training\n",
    "    content_features = self.get_features(self.content)\n",
    "    style_features = self.get_features(self.style)\n",
    "    # calculate the gram matrices for each layer of our style representation\n",
    "    style_grams = {layer: self.gram_matrix(style_features[layer]) for layer in style_features}\n",
    "    # create a third \"target\" image and prep it for change\n",
    "    # it is a good idea to start off with the target as a copy of our *content* image\n",
    "    # then iteratively change its style\n",
    "    target = self.content.clone().requires_grad_(True).to(self.device)\n",
    "    # weights for each style layer \n",
    "    # weighting earlier layers more will result in *larger* style artifacts\n",
    "    # notice we are excluding `conv4_2` our content representation\n",
    "    style_weights = {'conv1_1': 1.,\n",
    "                    'conv2_1': 0.75,\n",
    "                    'conv3_1': 0.2,\n",
    "                    'conv4_1': 0.2,\n",
    "                    'conv5_1': 0.2}\n",
    "    # Add target image parameters to optimizer\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "    # For displaying the target image, intermittently\n",
    "    if vis is True:\n",
    "      show_every = 400\n",
    "    # Update image e number of times (default = 2000)\n",
    "    for e in range(self.epoch):\n",
    "        # get the features from your target image\n",
    "        target_features = self.get_features(target)\n",
    "        # calculate the content loss: ((∑(T_c - C_c)^2) / 2) == (mean((T_c - C_c)^2))\n",
    "        content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "        # calculate the style loss\n",
    "        style_loss = 0\n",
    "        # then add to it for each layer's gram matrix loss\n",
    "        for layer in style_weights:\n",
    "            # get the \"target\" style representation for the layer\n",
    "            target_feature = target_features[layer]\n",
    "            # calculate the Gram Matrix of the target image\n",
    "            target_gram = self.gram_matrix(target_feature)\n",
    "            _, d, h, w = target_feature.shape\n",
    "            # get the \"style\" style representation\n",
    "            style_gram = style_grams[layer]\n",
    "            # the style loss for one layer, weighted appropriately\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            # normalize to the style loss\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "        # calculate the total loss: α(L_content) + β(L_style)\n",
    "        total_loss = self.content_weight * content_loss + self.style_weight * style_loss\n",
    "        # Zero the existing gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        # Gradient Descent Step\n",
    "        optimizer.step()\n",
    "        # Display intermediate images and print the loss\n",
    "        if vis is True and e % show_every == 0:\n",
    "            print('Total loss: ', total_loss.item())\n",
    "            plt.imshow(self.im_convert(target))\n",
    "            plt.show()\n",
    "    # Display content and final, target image\n",
    "    if vis is True:\n",
    "      _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "      ax1.imshow(self.im_convert(self.content))\n",
    "      ax2.imshow(self.im_convert(target))\n",
    "    # save the image to disk\n",
    "    self.save_image(self.im_convert(target))\n",
    "\n",
    "if 0:\n",
    "  n = Network('images/octopus.jpg', 'images/hockney.jpg', 'images/output.jpg')\n",
    "  n.transfer()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file 'images/output.jpg'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7fe6bc5a381c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/octopus.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images/hockney.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images/output.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-195577b4200a>\u001b[0m in \u001b[0;36mtransfer\u001b[0;34m(self, vis)\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# save the image to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-195577b4200a>\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \"\"\"\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# creating a image object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# save a image using extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2929\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2930\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 2931\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2932\u001b[0m     )\n\u001b[1;32m   2933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file 'images/output.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "n = Network('images/octopus.jpg', 'images/hockney.jpg', 'images/output.jpg')\n",
    "n.transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}