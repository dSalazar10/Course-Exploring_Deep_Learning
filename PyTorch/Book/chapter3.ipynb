{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n       ...,\n       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# File name\n",
    "wine_path = 'data/winequality-white.csv'\n",
    "# Data Type is float, delimiter is ';' as the file uses it to separate data\n",
    "# This also skips row 1 as that is the row containing the headers\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "wineq_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((4898, 12),\n ['fixed acidity',\n  'volatile acidity',\n  'citric acid',\n  'residual sugar',\n  'chlorides',\n  'free sulfur dioxide',\n  'total sulfur dioxide',\n  'density',\n  'pH',\n  'sulphates',\n  'alcohol',\n  'quality'])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Check that all the data has been read\n",
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
    "wineq_numpy.shape, col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row in the table is independent from the others and order doesn't matter. No column encoded information on what rows came before and what rows came after, and as such is a flat table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([4898, 12]), 'torch.FloatTensor')"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# convert from NumPy array to PyTorch Tensor\n",
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "wineq.shape, wineq.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n         ...,\n         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n torch.Size([4898, 11]))"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# strip the categorical column from the tensor\n",
    "data = wineq[:, :-1]\n",
    "data, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([6., 6., 6.,  ..., 6., 7., 6.]), torch.Size([4898]))"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Save just the categorical column from the tensro\n",
    "target = wineq[:, -1] \n",
    "target, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([6, 6, 6,  ..., 6, 7, 6])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Quantitative Labeling of Categorical Values\n",
    "target = wineq[:, -1].long()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 1., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Qualitative Labeling of Categorical Values\n",
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "# scatter_ takes 3 inputs:\n",
    "# 1) The dimension along which the following two arguments are specified\n",
    "# 2) A column tensor indicating the indices of the elements to scatter\n",
    "#   - unsqueeze - add an extra dummy_dimension to match dimensions\n",
    "# 3) A tensor containing the elements to scatter or a single scalar to scatter (1, in this case)\n",
    "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 1.7209e-01, -8.1764e-02,  2.1325e-01,  ..., -1.2468e+00,\n         -3.4914e-01, -1.3930e+00],\n        [-6.5743e-01,  2.1587e-01,  4.7991e-02,  ...,  7.3992e-01,\n          1.3467e-03, -8.2418e-01],\n        [ 1.4756e+00,  1.7448e-02,  5.4378e-01,  ...,  4.7502e-01,\n         -4.3677e-01, -3.3662e-01],\n        ...,\n        [-4.2042e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3131e+00,\n         -2.6152e-01, -9.0544e-01],\n        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0048e+00,\n         -9.6250e-01,  1.8574e+00],\n        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7502e-01,\n         -1.4882e+00,  1.0448e+00]])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# calculate the mean\n",
    "data_mean = torch.mean(data, dim=0)\n",
    "# calculate the std\n",
    "data_var = torch.var(data, dim=0)\n",
    "# normalize the data\n",
    "data_normalized = (data - data_mean) / torch.sqrt(data_var)\n",
    "data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([4898]), torch.bool, tensor(20))"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# We could determine which rows in target correspond to a score less than or equal to 3\n",
    "bad_indexes = torch.le(target, 3)\n",
    "bad_indexes.shape, bad_indexes.dtype, bad_indexes.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([20, 11])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# advanced indexing - use a binary tensor to index the data tensor\n",
    "bad_data = data[bad_indexes]\n",
    "bad_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "At first glance, the bad wines seem to have higher total sulfur dioxide, among other differences. You could use a threshold on total sulfur dioxide as a crude criterion for discriminating good wines from bad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 fixed acidity          7.60   6.89   6.73\n 1 volatile acidity       0.33   0.28   0.27\n 2 citric acid            0.34   0.34   0.33\n 3 residual sugar         6.39   6.71   5.26\n 4 chlorides              0.05   0.05   0.04\n 5 free sulfur dioxide   53.33  35.42  34.55\n 6 total sulfur dioxide 170.60 141.83 125.25\n 7 density                0.99   0.99   0.99\n 8 pH                     3.19   3.18   3.22\n 9 sulphates              0.47   0.49   0.50\n10 alcohol               10.34  10.26  11.42\n"
    }
   ],
   "source": [
    "# extract the three groups of data into their own slices\n",
    "bad_data = data[torch.le(target, 3)]\n",
    "mid_data = data[torch.gt(target, 3) & torch.lt(target, 7)] \n",
    "good_data = data[torch.ge(target, 7)]\n",
    "\n",
    "# Calculate the mean of each slice\n",
    "bad_mean = torch.mean(bad_data, dim=0) \n",
    "mid_mean = torch.mean(mid_data, dim=0) \n",
    "good_mean = torch.mean(good_data, dim=0)\n",
    "\n",
    "# enumerate the three slices\n",
    "for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n",
    "    print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args))ormat(i, *args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([4898]), torch.bool, tensor(2727))"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# midpoint of the total sulfur dioxide\n",
    "total_sulfur_threshold = 141.83\n",
    "# isolate the data with just the total sulfur dioxide (col 6)\n",
    "total_sulfur_data = data[:,6]\n",
    "# get the indexes in which the total sulfur dioxide column is below the mid-point\n",
    "predicted_indexes = torch.lt(total_sulfur_data, total_sulfur_threshold)\n",
    "predicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([4898]), torch.bool, tensor(3258))"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# get the data that is labeled as the best wine\n",
    "actual_indexes = torch.gt(target, 5)\n",
    "actual_indexes.shape, actual_indexes.dtype, actual_indexes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you have about 500 more good wines than your threshold predicted, you already have hard evidence that the threshold isn’t perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2018, 0.74000733406674, 0.6193984039287906)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# compare est with act to determine how well you did\n",
    "n_matches = torch.sum(actual_indexes & predicted_indexes).item()\n",
    "n_predicted = torch.sum(predicted_indexes).item()\n",
    "n_actual = torch.sum(actual_indexes).item()\n",
    "n_matches, n_matches / n_predicted, n_matches / n_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You correctly match 2018 records! That resulted in a 74% accuracy in your single variable prediction.\n",
    "Unfortunately, the correct score is only 61%.\n",
    "This shows that making a predicition only using a single variable creates a bad prediction method.\n",
    "This can be overcome using a layered Deep Learning model, but it is a good example of how to use PyTorch to perform Data Exploration with Tableu data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n         1.6000e+01],\n        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n         4.0000e+01],\n        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n         3.2000e+01],\n        ...,\n        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n         9.0000e+01],\n        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n         6.1000e+01],\n        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n         4.9000e+01]])"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "bikes_path = 'data/hour-fixed.csv'\n",
    "# Convert date strings to numbers corresponding to the day of the month in column 1.\n",
    "bikes_numpy = np.loadtxt(bikes_path,dtype=np.float32,delimiter=\",\",skiprows=1,converters={1: lambda x: float(x[8:10])})\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([17520, 17]), (17, 1))"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "# 17,520 hours, 17 columns\n",
    "bikes.shape, bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([730, 24, 17]), (408, 17, 1))"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# reshape the data to have three axes (day, hour, and then your 17 columns)\n",
    "# -1 means the row is inferred, and results in 730\n",
    "# bikes.shape[1] = 17\n",
    "# 24 * 17 = 408 -> the number of strides for for row 1\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "# Row 1 is the day, row 2 is the hour, and row 3 is the data\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([730, 17, 24]), (408, 1, 17))"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# In order to get the proper NxCxL order, you must trasnpose (day, 17 columns, hour)\n",
    "daily_bikes = daily_bikes.transpose(1, 2)\n",
    "# think of this as each page is the day, and that page contains a table of the hourly data for the day\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "limit yourself to the first day for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "# initialize a zero-filled matrix with a number of rows equal to the number of hours in the day\n",
    "# and a number of columns equal to the number of weather levels\n",
    "first_day = bikes[:24].long()\n",
    "weather_onehot = torch.zeros(first_day.shape[0], 4)\n",
    "first_day[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "# complete the one-hot encoding\n",
    "# You’re decreasing the values by 1 in index because the weather situation ranges from 1 to 4,\n",
    "# whereas indices are 0-based.\n",
    "weather_onehot.scatter_(dim=1, index=first_day[:,9].unsqueeze(1) - 1, value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,\n          0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,\n         16.0000,  1.0000,  0.0000,  0.0000,  0.0000]])"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "# concatenate the one-hot data with the whole data for the one day\n",
    "torch.cat((bikes[:24], weather_onehot), 1)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([730, 4, 24])"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "# Alternatively you could have used the daily_bikes\n",
    "daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])\n",
    "daily_weather_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([730, 4, 24])"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "# then scatter\n",
    "daily_weather_onehot.scatter_(1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)\n",
    "daily_weather_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate\n",
    "daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is ordinal data, we could normalize it\n",
    "daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) / 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple ways to rescale the variables, depending on your NN needs\n",
    "temp = daily_bikes[:, 10, :]\n",
    "temp_min = torch.min(temp)\n",
    "temp_max = torch.max(temp)\n",
    "daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - temp_min) / (temp_max - temp_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = daily_bikes[:, 10, :]\n",
    "daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - torch.mean(temp)) / torch.std(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "jane_path = 'data/1342-0.txt'\n",
    "with open(jane_path, encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "# split your text into a list of lines and pick an arbitrary line to focus on\n",
    "lines = text.split('\\n')\n",
    "line = lines[200]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([70, 128])"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "# Create a tensor that can hold the total number of one-hot encoded characters for the whole line\n",
    "# 128 hardcoded due to the limits of ASCII\n",
    "letter_tensor = torch.zeros(len(line), 128)\n",
    "letter_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    # The text uses directional double quotes, which aren’t valid ASCII, so screen them out here.\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    letter_tensor[i][letter_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('“Impossible, Mr. Bennet, impossible, when I am not acquainted with him',\n ['impossible',\n  'mr',\n  'bennet',\n  'impossible',\n  'when',\n  'i',\n  'am',\n  'not',\n  'acquainted',\n  'with',\n  'him'])"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "def clean_words(input_str):\n",
    "    \"\"\"\n",
    "    This converts all letters to lower case, replaces any new lines with\n",
    "    a space, and splits on the words.\n",
    "    Then it removes the punctuation for all words in the list\n",
    "    Input:\n",
    "    * input_str: a string of ascii characters, representing a sentence\n",
    "    Output:\n",
    "    * returns a clean list of words based on the input sentence\n",
    "    \"\"\"\n",
    "    punctuation = '.,;:\"!?”“_-'\n",
    "    word_list = input_str.lower().replace('\\n',' ').split()\n",
    "    word_list = [word.strip(punctuation) for word in word_list]\n",
    "    return word_list\n",
    "\n",
    "words_in_line = clean_words(line)\n",
    "line, words_in_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7261, 3394)"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "# build a mapping of words to indexes in your encoding\n",
    "# Sort the cleansed words of the whole text file\n",
    "word_list = sorted(set(clean_words(text)))\n",
    "# create a key:value pair dictionary using key=word and value=index\n",
    "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
    "# 7261 words, impossible shows up 3394 times\n",
    "len(word2index_dict), word2index_dict['impossible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 3394 impossible\n 1 4305 mr\n 2  813 bennet\n 3 3394 impossible\n 4 7078 when\n 5 3315 i\n 6  415 am\n 7 4436 not\n 8  239 acquainted\n 9 7148 with\n10 3215 him\ntorch.Size([11, 7261])\n"
    }
   ],
   "source": [
    "# create a zero tensor to hold the dictionary for the one liner\n",
    "word_tensor = torch.zeros(len(words_in_line), len(word2index_dict))\n",
    "# create key;value pairs for the one liner\n",
    "for i, word in enumerate(words_in_line):\n",
    "    # populate the records in the dictionary\n",
    "    word_index = word2index_dict[word]\n",
    "    word_tensor[i][word_index] = 1\n",
    "    print('{:2} {:4} {}'.format(i, word_index, word))\n",
    "# one sentence of length 11 in an encoding space of size 7261—the number of words in your dictionary\n",
    "print(word_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images\n",
    "img is a NumPy array-like object with three dimensions: two spatial dimensions (width and height) and a third dimension corresponding to the channels red, green, and blue. (W x H x C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(720, 1280, 3)"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "import imageio\n",
    "img_arr = imageio.imread('data/bobby.jpg')\n",
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch modules that deal with image data require tensors to be laid out as C x H x W (channels, height, and width, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img_arr)\n",
    "out = torch.transpose(img, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can preallocate a tensor of appropriate size and fill it with images loaded from a directory\n",
    "batch_size = 100\n",
    "# N(100) x C(3) x H(256) x W(256)\n",
    "batch = torch.zeros(100, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all png images from an input directory and store them in the tensor\n",
    "import os\n",
    "data_dir = 'data/image-cats/'\n",
    "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name) == '.png']\n",
    "for i, filename in enumerate(filenames):\n",
    "    img_arr = imageio.imread(filename)\n",
    "    batch[i] = torch.transpose(torch.from_numpy(img_arr), 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical thing that you’ll want to do is cast a tensor to floating-point and normalize the values of the pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One possibility is to divide the values of pixels by 255\n",
    "batch = batch.float()\n",
    "batch /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another possibility is to compute mean and standard deviation of the input data and scale\n",
    "# it so that the output has zero mean and unit standard deviation across each channel\n",
    "n_channels = batch.shape[1]\n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])\n",
    "    batch[:, c] = (batch[:, c] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volumetric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Reading DICOM (examining files):1/99 files (1.0%99/99 files (100.0%)\n  Found 1 correct series.\nReading DICOM (loading data):74/99  (74.799/99  (100.0%)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(99, 512, 512)"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "import imageio\n",
    "dir_path = \"data/images-lungs/\" \n",
    "# Load a sample CT scan by using the volread function in the imageio module\n",
    "# which takes a directory as argument and assembles all DICOM files in a \n",
    "# series in a NumPy 3D array\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the layout is different from what PyTorch expects, due to the lack of channel information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 512, 512, 99])"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "vol = torch.from_numpy(vol_arr).float()\n",
    "vol = torch.transpose(vol, 0, 2)\n",
    "# make room for the channel dimension by using unsqueeze\n",
    "vol = torch.unsqueeze(vol, 0)\n",
    "# C x H x W x N\n",
    "vol.shape\n",
    "# At this point, you could assemble a 5D data set by stacking multiple volumes along the batch direction\n",
    "# This example is just with a single volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(44100, array([ -388, -3387, -4634, ...,  2289,  1327,    90], dtype=int16))"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "import scipy.io.wavfile as wavfile\n",
    "wave_path = 'data/1-100038-A-14.wav'\n",
    "# returns two outputs, namely the sampling frequency and the waveform as a 16-bit integer 1D array\n",
    "freq, waveform_arr = wavfile.read(wave_path)\n",
    "freq, waveform_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a single 1D array, which tells us that it's a mono recording - we'd have two waveforms (two channels) if the sound were stereo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([220500])"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "waveform = torch.from_numpy(waveform_arr).float()\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For architectures based on filtering the 1D signal with cascades of learned filter banks, such as convolutional networks, we would need to lay out the tensor as `N x C x L`, where `N` is the number of sounds in a dataset, `C` the number of channels and `L` the number of samples in time.\n",
    "\n",
    "Conversely, for architectures that incorporate the notion of temporal sequences, just as recurrent networks we mentioned for text, data needs to be laid out as `L x N x C` - sequence length comes first. Intuitively, this is because the latter architectures take one set of `C` values at a time - the signal is not considered as a whole, but as an individual input changing in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([129, 984])"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "from scipy import signal\n",
    "# Convert the audio file into a spectrogram\n",
    "f_arr, t_arr, sp_arr = signal.spectrogram(waveform_arr, freq)\n",
    "# Convert the resulting NumPy frequency array into a Tensor\n",
    "sp_mono = torch.from_numpy(sp_arr)\n",
    "sp_mono.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([129, 984]), torch.Size([129, 984]))"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "# convert the three spectrograms into Tensors\n",
    "sp_left = sp_right = sp_arr\n",
    "sp_left_t = torch.from_numpy(sp_left)\n",
    "sp_right_t = torch.from_numpy(sp_right)\n",
    "sp_left_t.shape, sp_right_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 129, 984])"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "# Stack the Tensors into a single spectrogram\n",
    "sp_t = torch.stack((sp_left_t, sp_right_t), dim=0)\n",
    "sp_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to build a dataset to use as input for a network, we will stack multiple spectrograms representing multiple sounds in a dataset along the first dimension, leading to a `N x C x F x T` tensor.\n",
    "\n",
    "Such tensor is indistinguishable from what we would build for a dataset set of images, where `F` is represents rows and `T` columns of an image. Indeed, we would tackle a sound classification problem on spectrograms with the exact same networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'plugin': 'ffmpeg',\n 'nframes': inf,\n 'ffmpeg_version': '4.2.2 built with Apple clang version 11.0.0 (clang-1100.0.33.8)',\n 'codec': 'h264',\n 'pix_fmt': 'yuv444p',\n 'fps': 20.0,\n 'source_size': (1280, 720),\n 'size': (1280, 720),\n 'duration': 14.0}"
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "import imageio\n",
    "video_path = 'data/cockatoo.mp4'\n",
    "# create a reader instance for the video\n",
    "reader = imageio.get_reader(video_path)\n",
    "meta = reader.get_meta_data()\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "n_channels = 3\n",
    "n_frames = meta['nframes']\n",
    "video = torch.empty(n_channels, n_frames, *meta['size']\n",
    "ideo.shape\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the reader and set the values for all three channels into in the proper `i`-th time slice\n",
    "for i, frame_arr in enumerate(reader):\n",
    "    frame = torch.from_numpy(frame_arr).float()\n",
    "    n_channels = 3\n",
    "    n_frames = meta['nframes']\n",
    "    video = torch.empty(n_channels, n_frames, (1280, 720))\n",
    "\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we iterate over individual frames and set each frame in the `C x T x H x W` video tensor, after transposing the channel. We can then obtain a batch by stacking multiple 4D tensors or pre-allocating a 5D tensor with a known batch size and filling it iteratively, clip by clip, assuming clips are trimmed to a fixed number of frames.\n",
    "\n",
    "Equating video data to volumetric data is not the only way to represent video for training purposes. This is a valid strategy if we deal with video bursts of fixed length. An alternative strategy is to resort to network architectures capable of processing long sequences and exploiting short and long-term relationships in time, just like for text or audio.\n",
    "// We'll see this kind of architectures when we take on recurrent networks.\n",
    "\n",
    "This next approach accounts for time along the batch dimension. Hence, we'll build our dataset as a 4D tensor, stacking frame by frame in the batch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_video = torch.empty(n_frames, n_channels, *meta['size'])\n",
    "\n",
    "for i, frame in enumerate(reader):\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    time_video[i] = torch.transpose(frame, 0, 2)\n",
    "\n",
    "time_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}